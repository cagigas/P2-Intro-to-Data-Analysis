{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1237"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_engagement = pd.read_csv('/Users/dgcagigas/Documents/DataAnalyzer/P2/daily_engagement.csv')\n",
    "len(daily_engagement['acct'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_employment(countries, employment):\n",
    "    max_country = None\n",
    "    max_employment = 0\n",
    "    for i in range(len(countries)):\n",
    "        country = countries[i]\n",
    "        country_employment = employment[i]\n",
    "        \n",
    "        if country_employment < max_employment:\n",
    "            max_country = country\n",
    "            max_employment = country_employment\n",
    "    return(max_country, max_employment)\n",
    "        \n",
    "def max_employment2(countries, employment):\n",
    "    i = employment.argmax()\n",
    "    return(countries[i], employment[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Afghanistan' 'Albania' 'Algeria']\n"
     ]
    }
   ],
   "source": [
    "countries = np.array([\n",
    "    'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina',\n",
    "    'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas',\n",
    "    'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',\n",
    "    'Belize', 'Benin', 'Bhutan', 'Bolivia',\n",
    "    'Bosnia and Herzegovina'\n",
    "])\n",
    "employment = np.array([\n",
    "    55.70000076,  51.40000153,  50.5       ,  75.69999695,\n",
    "    58.40000153,  40.09999847,  61.5       ,  57.09999847,\n",
    "    60.90000153,  66.59999847,  60.40000153,  68.09999847,\n",
    "    66.90000153,  53.40000153,  48.59999847,  56.79999924,\n",
    "    71.59999847,  58.40000153,  70.40000153,  41.20000076\n",
    "])\n",
    "print countries[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def overall_completion_rate(female_completion, male_completion):\n",
    "    return (female_completion+male_completion)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_completion = np.array([\n",
    "    97.35583,  104.62379,  103.02998,   95.14321,  103.69019,\n",
    "    98.49185,  100.88828,   95.43974,   92.11484,   91.54804,\n",
    "    95.98029,   98.22902,   96.12179,  119.28105,   97.84627,\n",
    "    29.07386,   38.41644,   90.70509,   51.7478 ,   95.45072\n",
    "])\n",
    "\n",
    "# Male school completion rate in 2007 for those 20 countries\n",
    "male_completion = np.array([\n",
    "     95.47622,  100.66476,   99.7926 ,   91.48936,  103.22096,\n",
    "     97.80458,  103.81398,   88.11736,   93.55611,   87.76347,\n",
    "    102.45714,   98.73953,   92.22388,  115.3892 ,   98.70502,\n",
    "     37.00692,   45.39401,   91.22084,   62.42028,   90.66958\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_data(values):\n",
    "    standarized_values = (values - values.mean()) / values.std()\n",
    "    return standarized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_time_for_paid_students(time_spent, days_to_cancel):\n",
    "#    is_paid = days_to_cance >= 7\n",
    "#    paid_time = time_spent[is_paid]\n",
    "#    return paid_time.mean()\n",
    "    return time_spent[days_to_cancel >= 7].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Time spent in the classroom in the first week for 20 students\n",
    "time_spent = np.array([\n",
    "       12.89697233,    0.        ,   64.55043217,    0.        ,\n",
    "       24.2315615 ,   39.991625  ,    0.        ,    0.        ,\n",
    "      147.20683783,    0.        ,    0.        ,    0.        ,\n",
    "       45.18261617,  157.60454283,  133.2434615 ,   52.85000767,\n",
    "        0.        ,   54.9204785 ,   26.78142417,    0.\n",
    "])\n",
    "\n",
    "# Days to cancel for 20 students\n",
    "days_to_cancel = np.array([\n",
    "      4,   5,  37,   3,  12,   4,  35,  38,   5,  37,   3,   3,  68,\n",
    "     38,  98,   2, 249,   2, 127,  35\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panda Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries = ['Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda',\n",
    "             'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan',\n",
    "             'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus',\n",
    "             'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia']\n",
    "\n",
    "life_expectancy_values = [74.7,  75. ,  83.4,  57.6,  74.6,  75.4,  72.3,  81.5,  80.2,\n",
    "                          70.3,  72.1,  76.4,  68.1,  75.2,  69.8,  79.4,  70.8,  62.7,\n",
    "                          67.3,  70.6]\n",
    "\n",
    "gdp_values = [ 1681.61390973,   2155.48523109,  21495.80508273,    562.98768478,\n",
    "              13495.1274663 ,   9388.68852258,   1424.19056199,  24765.54890176,\n",
    "              27036.48733192,   1945.63754911,  21721.61840978,  13373.21993972,\n",
    "                483.97086804,   9783.98417323,   2253.46411147,  25034.66692293,\n",
    "               3680.91642923,    366.04496652,   1175.92638695,   1132.21387981]\n",
    "\n",
    "# Life expectancy and gdp data in 2007 for 20 countries\n",
    "life_expectancy = pd.Series(life_expectancy_values)\n",
    "gdp = pd.Series(gdp_values)\n",
    "\n",
    "def variable_correlation(variable1, variable2):\n",
    "    '''\n",
    "    Fill in this function to calculate the number of data points for which\n",
    "    the directions of variable1 and variable2 relative to the mean are the\n",
    "    same, and the number of data points for which they are different.\n",
    "    Direction here means whether each value is above or below its mean.\n",
    "    \n",
    "    You can classify cases where the value is equal to the mean for one or\n",
    "    both variables however you like.\n",
    "    \n",
    "    Each argument will be a Pandas series.\n",
    "    \n",
    "    For example, if the inputs were pd.Series([1, 2, 3, 4]) and\n",
    "    pd.Series([4, 5, 6, 7]), then the output would be (4, 0).\n",
    "    This is because 1 and 4 are both below their means, 2 and 5 are both\n",
    "    below, 3 and 6 are both above, and 4 and 7 are both above.\n",
    "    \n",
    "    On the other hand, if the inputs were pd.Series([1, 2, 3, 4]) and\n",
    "    pd.Series([7, 6, 5, 4]), then the output would be (0, 4).\n",
    "    This is because 1 is below its mean but 7 is above its mean, and\n",
    "    so on.\n",
    "    '''\n",
    "    both_above = (variable1 > variable1.mean()) & (variable2 > variable2.mean())\n",
    "    both_below = (variable1 < variable1.mean()) & (variable2 < variable2.mean())\n",
    "    is_same_direction = both_above | both_below\n",
    "    \n",
    "\n",
    "    num_same_direction = is_same_direction.sum()        # Replace this with your code\n",
    "    num_different_direction = len(variable1) - num_same_direction  # Replace this with your code\n",
    "    \n",
    "    return (num_same_direction, num_different_direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_correlation(life_expectancy, gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries = [\n",
    "    'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina',\n",
    "    'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas',\n",
    "    'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium',\n",
    "    'Belize', 'Benin', 'Bhutan', 'Bolivia',\n",
    "    'Bosnia and Herzegovina'\n",
    "]\n",
    "\n",
    "\n",
    "employment_values = [\n",
    "    55.70000076,  51.40000153,  50.5       ,  75.69999695,\n",
    "    58.40000153,  40.09999847,  61.5       ,  57.09999847,\n",
    "    60.90000153,  66.59999847,  60.40000153,  68.09999847,\n",
    "    66.90000153,  53.40000153,  48.59999847,  56.79999924,\n",
    "    71.59999847,  58.40000153,  70.40000153,  41.20000076\n",
    "]\n",
    "\n",
    "# Employment data in 2007 for 20 countries\n",
    "employment = pd.Series(employment_values, index=countries)\n",
    "\n",
    "def max_employment(employment):\n",
    "    '''\n",
    "    Fill in this function to return the name of the country\n",
    "    with the highest employment in the given employment\n",
    "    data, and the employment in that country.\n",
    "    \n",
    "    The input will be a Pandas series where the values\n",
    "    are employment and the index is country names.\n",
    "    \n",
    "    Try using the Pandas argmax() function. Documention is\n",
    "    here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.argmax.html\n",
    "    '''\n",
    "    \n",
    "    max_country = employment.argmax()      # Replace this with your code\n",
    "    max_value = employment.loc[max_country]   # Replace this with your code\n",
    "\n",
    "    return (max_country, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     1.0\n",
       "b     2.0\n",
       "c    13.0\n",
       "d    24.0\n",
       "e    30.0\n",
       "f    40.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n",
    "s2 = pd.Series([10, 20, 30, 40], index=['c', 'd', 'e', 'f'])\n",
    "s1.add(s2, fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = pd.Series([\n",
    "    'Andre Agassi',\n",
    "    'Barry Bonds',\n",
    "    'Christopher Columbus',\n",
    "    'Daniel Defoe',\n",
    "    'Emilio Estevez',\n",
    "    'Fred Flintstone',\n",
    "    'Greta Garbo',\n",
    "    'Humbert Humbert',\n",
    "    'Ivan Ilych',\n",
    "    'James Joyce',\n",
    "    'Keira Knightley',\n",
    "    'Lois Lane',\n",
    "    'Mike Myers',\n",
    "    'Nick Nolte',\n",
    "    'Ozzy Osbourne',\n",
    "    'Pablo Picasso',\n",
    "    'Quirinus Quirrell',\n",
    "    'Rachael Ray',\n",
    "    'Susan Sarandon',\n",
    "    'Tina Turner',\n",
    "    'Ugueth Urbina',\n",
    "    'Vince Vaughn',\n",
    "    'Woodrow Wilson',\n",
    "    'Yoji Yamada',\n",
    "    'Zinedine Zidane'\n",
    "])\n",
    "def reverse_name(name):\n",
    "    split_name = name.split(\" \")\n",
    "    return split_name[1] + \", \" + split_name[0]\n",
    "\n",
    "def reverse_names(names):\n",
    "    '''\n",
    "    Fill in this function to return a new series where each name\n",
    "    in the input series has been transformed from the format\n",
    "    \"Firstname Lastname\" to \"Lastname, FirstName\".\n",
    "    \n",
    "    Try to use the Pandas apply() function rather than a loop.\n",
    "    '''\n",
    "    \n",
    "    return names.apply(reverse_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Agassi, Andre\n",
       "1              Bonds, Barry\n",
       "2     Columbus, Christopher\n",
       "3             Defoe, Daniel\n",
       "4           Estevez, Emilio\n",
       "5          Flintstone, Fred\n",
       "6              Garbo, Greta\n",
       "7          Humbert, Humbert\n",
       "8               Ilych, Ivan\n",
       "9              Joyce, James\n",
       "10         Knightley, Keira\n",
       "11               Lane, Lois\n",
       "12              Myers, Mike\n",
       "13              Nolte, Nick\n",
       "14           Osbourne, Ozzy\n",
       "15           Picasso, Pablo\n",
       "16       Quirrell, Quirinus\n",
       "17             Ray, Rachael\n",
       "18          Sarandon, Susan\n",
       "19             Turner, Tina\n",
       "20           Urbina, Ugueth\n",
       "21            Vaughn, Vince\n",
       "22          Wilson, Woodrow\n",
       "23             Yamada, Yoji\n",
       "24         Zidane, Zinedine\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_names(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /datasets/ud170/gapminder/employment_above_15.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e3a0e680503c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# learn about DataFrames next lesson.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0memployment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'employment_above_15.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfemale_completion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'female_completion_rate.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmale_completion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'male_completion_rate.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /datasets/ud170/gapminder/employment_above_15.csv does not exist"
     ]
    }
   ],
   "source": [
    "\n",
    "# The following code reads all the Gapminder data into Pandas DataFrames. You'll\n",
    "# learn about DataFrames next lesson.\n",
    "\n",
    "employment = pd.read_csv(path + 'employment_above_15.csv', index_col='Country')\n",
    "female_completion = pd.read_csv(path + 'female_completion_rate.csv', index_col='Country')\n",
    "male_completion = pd.read_csv(path + 'male_completion_rate.csv', index_col='Country')\n",
    "life_expectancy = pd.read_csv(path + 'life_expectancy.csv', index_col='Country')\n",
    "gdp = pd.read_csv(path + 'gdp_per_capita.csv', index_col='Country')\n",
    "\n",
    "# The following code creates a Pandas Series for each variable for the United States.\n",
    "# You can change the string 'United States' to a country of your choice.\n",
    "\n",
    "employment_us = employment.loc['United States']\n",
    "female_completion_us = female_completion.loc['United States']\n",
    "male_completion_us = male_completion.loc['United States']\n",
    "life_expectancy_us = life_expectancy.loc['United States']\n",
    "gdp_us = gdp.loc['United States']\n",
    "\n",
    "# Uncomment the following line of code to see the available country names\n",
    "print employment.index.values\n",
    "\n",
    "# Use the Series defined above to create a plot of each variable over time for\n",
    "# the country of your choice. You will only be able to display one plot at a time\n",
    "# with each \"Test Run\"\n",
    "%pylab inline\n",
    "employment_us.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subway ridership for 5 stations on 10 different days\n",
    "ridership = np.array([\n",
    "    [   0,    0,    2,    5,    0],\n",
    "    [1478, 3877, 3674, 2328, 2539],\n",
    "    [1613, 4088, 3991, 6461, 2691],\n",
    "    [1560, 3392, 3826, 4787, 2613],\n",
    "    [1608, 4802, 3932, 4477, 2705],\n",
    "    [1576, 3933, 3909, 4979, 2685],\n",
    "    [  95,  229,  255,  496,  201],\n",
    "    [   2,    0,    1,   27,    0],\n",
    "    [1438, 3785, 3589, 4174, 2215],\n",
    "    [1342, 4043, 4009, 4665, 3033]\n",
    "])\n",
    "\n",
    "def mean_riders_for_max_station(ridership):\n",
    "    '''\n",
    "    Fill in this function to find the station with the maximum riders on the\n",
    "    first day, then return the mean riders per day for that station. Also\n",
    "    return the mean ridership overall for comparsion.\n",
    "    \n",
    "    Hint: NumPy's argmax() function might be useful:\n",
    "    http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n",
    "    '''\n",
    "    max_station = ridership[0,:].argmax()\n",
    "    overall_mean = ridership.mean()\n",
    "    mean_for_max = ridership[:,max_station].mean()\n",
    "    \n",
    "    return (overall_mean, mean_for_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_and_max_riders_per_day(ridership):\n",
    "    '''\n",
    "    Fill in this function. First, for each subway station, calculate the\n",
    "    mean ridership per day. Then, out of all the subway stations, return the\n",
    "    maximum and minimum of these values. That is, find the maximum\n",
    "    mean-ridership-per-day and the minimum mean-ridership-per-day for any\n",
    "    subway station.\n",
    "    '''\n",
    "    station_riders = riders.mean(axis = 0)\n",
    "    max_daily_ridership = station_riders.max()     # Replace this with your code\n",
    "    min_daily_ridership = station_riders.min()     # Replace this with your code\n",
    "    \n",
    "    return (max_daily_ridership, min_daily_ridership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_riders_for_max_station(ridership):\n",
    "    '''\n",
    "    Fill in this function to find the station with the maximum riders on the\n",
    "    first day, then return the mean riders per day for that station. Also\n",
    "    return the mean ridership overall for comparsion.\n",
    "    \n",
    "    This is the same as a previous exercise, but this time the\n",
    "    input is a Pandas DataFrame rather than a 2D NumPy array.\n",
    "    '''\n",
    "    max_station = ridership.iloc[0].argmax()\n",
    "    overall_mean = ridership.values.mean()\n",
    "    mean_for_max = ridership[max_station].mean()\n",
    "\n",
    "    return (overall_mean, mean_for_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correlation(x, y):\n",
    "    std_x = (x - x.mean()) / x.std(ddof=0)\n",
    "    std_y = (y - y.mean()) / y.std(ddof=0)\n",
    "    \n",
    "    return (std_x * std_y).mean()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cumulative entries and exits for one station for a few hours.\n",
    "entries_and_exits = pd.DataFrame({\n",
    "    'ENTRIESn': [3144312, 3144335, 3144353, 3144424, 3144594,\n",
    "                 3144808, 3144895, 3144905, 3144941, 3145094],\n",
    "    'EXITSn': [1088151, 1088159, 1088177, 1088231, 1088275,\n",
    "               1088317, 1088328, 1088331, 1088420, 1088753]\n",
    "})\n",
    "\n",
    "def get_hourly_entries_and_exits(entries_and_exits):\n",
    "    '''\n",
    "    Fill in this function to take a DataFrame with cumulative entries\n",
    "    and exits (entries in the first column, exits in the second) and\n",
    "    return a DataFrame with hourly entries and exits (entries in the\n",
    "    first column, exits in the second).\n",
    "    '''\n",
    "    return entries_and_exits - entries_and_exits.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grades_df = pd.DataFrame(\n",
    "    data={'exam1': [43, 81, 78, 75, 89, 70, 91, 65, 98, 87],\n",
    "          'exam2': [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]},\n",
    "    index=['Andre', 'Barry', 'Chris', 'Dan', 'Emilio', \n",
    "           'Fred', 'Greta', 'Humbert', 'Ivan', 'James']\n",
    ")\n",
    "def convert_grade(grade):\n",
    "    if grade >= 90:\n",
    "        return 'A'\n",
    "    elif grade >= 80:\n",
    "        return 'B'\n",
    "    elif grade >= 70:\n",
    "        return 'C'\n",
    "    elif grade >= 60:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'F'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_grades(grades):\n",
    "    '''\n",
    "    Fill in this function to convert the given DataFrame of numerical\n",
    "    grades to letter grades. Return a new DataFrame with the converted\n",
    "    grade.\n",
    "    \n",
    "    The conversion rule is:\n",
    "        90-100 -> A\n",
    "        80-89  -> B\n",
    "        70-79  -> C\n",
    "        60-69  -> D\n",
    "        0-59   -> F\n",
    "    '''\n",
    "    \n",
    "    return grades.applymap(convert_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_column(column):\n",
    "    return (column - column.mean()) / column.std()\n",
    "\n",
    "def standardize(df):\n",
    "    '''\n",
    "    Fill in this function to standardize each column of the given\n",
    "    DataFrame. To standardize a variable, convert each value to the\n",
    "    number of standard deviations it is above or below the mean.\n",
    "    '''\n",
    "    return df.apply(standardize_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_largest_column(column):\n",
    "    '''\n",
    "    Fill in this function to return the second-largest value of each \n",
    "    column of the input DataFrame.\n",
    "    '''\n",
    "    sorted_column = column.sort_values(ascending = False)\n",
    "    return sorted_column.iloc[1]\n",
    "\n",
    "def second_largest(df):\n",
    "    '''\n",
    "    Fill in this function to return the second-largest value of each \n",
    "    column of the input DataFrame.\n",
    "    '''\n",
    "    return df.apply(second_largest_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Andre</th>\n",
       "      <th>Barry</th>\n",
       "      <th>Chris</th>\n",
       "      <th>Dan</th>\n",
       "      <th>Emilio</th>\n",
       "      <th>Fred</th>\n",
       "      <th>Greta</th>\n",
       "      <th>Humbert</th>\n",
       "      <th>Ivan</th>\n",
       "      <th>James</th>\n",
       "      <th>exam1</th>\n",
       "      <th>exam2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Andre</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barry</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chris</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emilio</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fred</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greta</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humbert</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ivan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>James</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Andre  Barry  Chris  Dan  Emilio  Fred  Greta  Humbert  Ivan  James  \\\n",
       "Andre      NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Barry      NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Chris      NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Dan        NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Emilio     NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Fred       NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Greta      NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Humbert    NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "Ivan       NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "James      NaN    NaN    NaN  NaN     NaN   NaN    NaN      NaN   NaN    NaN   \n",
       "\n",
       "         exam1  exam2  \n",
       "Andre      NaN    NaN  \n",
       "Barry      NaN    NaN  \n",
       "Chris      NaN    NaN  \n",
       "Dan        NaN    NaN  \n",
       "Emilio     NaN    NaN  \n",
       "Fred       NaN    NaN  \n",
       "Greta      NaN    NaN  \n",
       "Humbert    NaN    NaN  \n",
       "Ivan       NaN    NaN  \n",
       "James      NaN    NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades_df = pd.DataFrame(\n",
    "    data={'exam1': [43, 81, 78, 75, 89, 70, 91, 65, 98, 87],\n",
    "          'exam2': [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]},\n",
    "    index=['Andre', 'Barry', 'Chris', 'Dan', 'Emilio', \n",
    "           'Fred', 'Greta', 'Humbert', 'Ivan', 'James']\n",
    ")\n",
    "\n",
    "def standardize(df):\n",
    "    '''\n",
    "    Fill in this function to standardize each column of the given\n",
    "    DataFrame. To standardize a variable, convert each value to the\n",
    "    number of standard deviations it is above or below the mean.\n",
    "    \n",
    "    This time, try to use vectorized operations instead of apply().\n",
    "    You should get the same results as you did before.\n",
    "    '''\n",
    "    return None\n",
    "\n",
    "def standardize_rows(df):\n",
    "    '''\n",
    "    Optional: Fill in this function to standardize each row of the given\n",
    "    DataFrame. Again, try not to use apply().\n",
    "    \n",
    "    This one is more challenging than standardizing each column!\n",
    "    '''\n",
    "    return None\n",
    "mean_diffs = grades_df.sub(grades_df.mean(axis='columns'), axis='index')\n",
    "mean_diffs.div(grades_df.std(axis = 'columns'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/datasets/ud170/subway/nyc_subway_weather.csv'\n",
    "subway_df = pd.read_csv(filename)\n",
    "\n",
    "subway_df.head()\n",
    "\n",
    "ridership_by_day = subway_df.groupby('day_week').mean()['ENTRIESn_hourly'] #Ordenamos por dia de la semana e imprimimos\n",
    "ridership_by_day.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRIESn</th>\n",
       "      <th>EXITSn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29.0</td>\n",
       "      <td>205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>71.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>132.0</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>170.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENTRIESn  EXITSn\n",
       "0       NaN     NaN\n",
       "1       NaN     NaN\n",
       "2      23.0     8.0\n",
       "3      14.0     8.0\n",
       "4      18.0    18.0\n",
       "5      29.0   205.0\n",
       "6      71.0    54.0\n",
       "7     132.0   593.0\n",
       "8     170.0    44.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = np.array([1, 3, 2, 4, 1, 6, 4])\n",
    "example_df = pd.DataFrame({\n",
    "    'value': values,\n",
    "    'even': values % 2 == 0,\n",
    "    'above_three': values > 3 \n",
    "}, index=['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "# DataFrame with cumulative entries and exits for multiple stations\n",
    "ridership_df = pd.DataFrame({\n",
    "    'UNIT': ['R051', 'R079', 'R051', 'R079', 'R051', 'R079', 'R051', 'R079', 'R051'],\n",
    "    'TIMEn': ['00:00:00', '02:00:00', '04:00:00', '06:00:00', '08:00:00', '10:00:00', '12:00:00', '14:00:00', '16:00:00'],\n",
    "    'ENTRIESn': [3144312, 8936644, 3144335, 8936658, 3144353, 8936687, 3144424, 8936819, 3144594],\n",
    "    'EXITSn': [1088151, 13755385,  1088159, 13755393,  1088177, 13755598, 1088231, 13756191,  1088275]\n",
    "})\n",
    "def hourly_for_group(entries_and_exits):\n",
    "    return entries_and_exits - entries_and_exits.shift(1)\n",
    "def get_hourly_entries_and_exits(entries_and_exits):\n",
    "    '''\n",
    "    Fill in this function to take a DataFrame with cumulative entries\n",
    "    and exits and return a DataFrame with hourly entries and exits.\n",
    "    The hourly entries and exits should be calculated separately for\n",
    "    each station (the 'UNIT' column).\n",
    "    \n",
    "    Hint: Use the `get_hourly_entries_and_exits()` function you wrote\n",
    "    in a previous quiz, DataFrame Vectorized Operations, and the `.apply()`\n",
    "    function, to help solve this problem.\n",
    "    '''\n",
    "    return None\n",
    "ridership_df.groupby('UNIT')[['ENTRIESn', 'EXITSn']].apply(hourly_for_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "subway_df = pd.DataFrame({\n",
    "    'UNIT': ['R003', 'R003', 'R003', 'R003', 'R003', 'R004', 'R004', 'R004',\n",
    "             'R004', 'R004'],\n",
    "    'DATEn': ['05-01-11', '05-02-11', '05-03-11', '05-04-11', '05-05-11',\n",
    "              '05-01-11', '05-02-11', '05-03-11', '05-04-11', '05-05-11'],\n",
    "    'hour': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'ENTRIESn': [ 4388333,  4388348,  4389885,  4391507,  4393043, 14656120,\n",
    "                 14656174, 14660126, 14664247, 14668301],\n",
    "    'EXITSn': [ 2911002,  2911036,  2912127,  2913223,  2914284, 14451774,\n",
    "               14451851, 14454734, 14457780, 14460818],\n",
    "    'latitude': [ 40.689945,  40.689945,  40.689945,  40.689945,  40.689945,\n",
    "                  40.69132 ,  40.69132 ,  40.69132 ,  40.69132 ,  40.69132 ],\n",
    "    'longitude': [-73.872564, -73.872564, -73.872564, -73.872564, -73.872564,\n",
    "                  -73.867135, -73.867135, -73.867135, -73.867135, -73.867135]\n",
    "})\n",
    "\n",
    "weather_df = pd.DataFrame({\n",
    "    'DATEn': ['05-01-11', '05-01-11', '05-02-11', '05-02-11', '05-03-11',\n",
    "              '05-03-11', '05-04-11', '05-04-11', '05-05-11', '05-05-11'],\n",
    "    'hour': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'latitude': [ 40.689945,  40.69132 ,  40.689945,  40.69132 ,  40.689945,\n",
    "                  40.69132 ,  40.689945,  40.69132 ,  40.689945,  40.69132 ],\n",
    "    'longitude': [-73.872564, -73.867135, -73.872564, -73.867135, -73.872564,\n",
    "                  -73.867135, -73.872564, -73.867135, -73.872564, -73.867135],\n",
    "    'pressurei': [ 30.24,  30.24,  30.32,  30.32,  30.14,  30.14,  29.98,  29.98,\n",
    "                   30.01,  30.01],\n",
    "    'fog': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'rain': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    'tempi': [ 52. ,  52. ,  48.9,  48.9,  54. ,  54. ,  57.2,  57.2,  48.9,  48.9],\n",
    "    'wspdi': [  8.1,   8.1,   6.9,   6.9,   3.5,   3.5,  15. ,  15. ,  15. ,  15. ]\n",
    "})\n",
    "\n",
    "def combine_dfs(subway_df, weather_df):\n",
    "    '''\n",
    "    Fill in this function to take 2 DataFrames, one with subway data and one with weather data,\n",
    "    and return a single dataframe with one row for each date, hour, and location. Only include\n",
    "    times and locations that have both subway data and weather data available.\n",
    "    '''\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
